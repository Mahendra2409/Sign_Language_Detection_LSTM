{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d224192e7d48e60",
   "metadata": {},
   "source": [
    "## 1.Import and Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34b7289ef5477775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c6b14439b07333",
   "metadata": {},
   "source": [
    "## 2.Keypoints using MP Holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97df0fe8443d41e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic  #holistic\n",
    "mp_drawing = mp.solutions.drawing_utils  #drawing utilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb39ccc0e49b59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image,model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  #COLOR CONVERSION BGR TO RGB\n",
    "    image.flags.writeable = False                   #Image is no loger writable\n",
    "    results = model.process(image)                  #Make Prediction\n",
    "    image.flags.writeable = True                    #Image is now writable\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)  #COLOR CONVERSION RGB TO BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a7cb4a64450639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmark(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS)  #Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)  #Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) #Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) #Draw right hand connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499766fab8df0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmark(image, results):\n",
    "    #Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS,\n",
    "                              mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "                              mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1))\n",
    "    #Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                              mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2))\n",
    "    #Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=2),\n",
    "                              mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2))\n",
    "    #Draw right hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2),\n",
    "                              mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a2fb5fcc98d758",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "mp_drawing.draw_landmarks??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6556796-835e-4571-ae37-ded42f3d47cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cap=cv2.VideoCapture(0)\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        #Read feed\n",
    "        ret, frame= cap.read()\n",
    "\n",
    "        #Make detection\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "\n",
    "        #Draw Lanmarks\n",
    "        draw_styled_landmark(image, results)\n",
    "\n",
    "        #Show to screeen\n",
    "        cv2.imshow('Opencv Feed',image)\n",
    "\n",
    "        #Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF==ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee0636341ab3094",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_styled_landmark(frame, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1f27567dff8939",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8e2c1913a26654",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6aae95c-b62e-4c91-82f1-efce72bf252e",
   "metadata": {},
   "source": [
    "## 3. Extract keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2028c35a925e2398",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "[len(results.pose_landmarks.landmark), len(results.face_landmarks.landmark), len(results.right_hand_landmarks.landmark)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2042fd06-4554-4d5b-81d2-c0df31724870",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results.pose_landmarks.landmark[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aca805-8c8e-4ce9-9057-35c114abb993",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose = []\n",
    "for res in results.pose_landmarks.landmark:\n",
    "    test = np.array([res.x, res.y, res.z, res.visibility])\n",
    "    pose.append(test)\n",
    "pose = np.array(pose).flatten() #Converting list to np array and flatten also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e418747b-9506-42c5-8761-38a7f3789542",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Error handeling (when no hand in frame)\n",
    "lh = []\n",
    "if results.left_hand_landmarks: \n",
    "    for res in results.left_hand_landmarks.landmark:\n",
    "        test = np.array([res.x, res.y, res.z])\n",
    "        lh.append(test)\n",
    "    lh = np.array(lh).flatten()\n",
    "else:\n",
    "    lh = np.zeros(21*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaa3bb9-520e-478a-9f8e-9300ad29c89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cc881b-7544-4c3f-afb9-3b7a9a0306c8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "rh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce66a743-5293-42bb-82f6-9199fcbffb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5a7a91-1253-4706-a48e-cdd68e671778",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test = extract_keypoints(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547e1e9f-244a-451d-bbf3-5ce9c7d7725c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f98d95c-1e7f-4e05-9972-ee4073cdbeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "33*4+468*3+21*3+21*3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90985856-ecf6-4203-9e01-9e84fba135ae",
   "metadata": {},
   "source": [
    "## 4. Setup folder for Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0430511-332e-407e-82a4-a119cbef9ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays (floder for saving data)\n",
    "DATA_PATH = os.path.join('MP_Data') \n",
    "\n",
    "# Actions that we try to detect (folder for each class name)\n",
    "actions = np.array(['hello', 'thanks', 'iloveyou'])\n",
    "\n",
    "# Thirty videos worth of data (no. of videos in each class and making a folder for each video)\n",
    "no_sequences = 30\n",
    "\n",
    "# Videos are going to be 30 frames in length (no. fo frames in each video)\n",
    "sequence_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00326907-42b7-443b-83b5-4e13c39f4f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MP Data\n",
    "    #hello\n",
    "        #1\n",
    "            #1-30\n",
    "        #2\n",
    "            #1-30\n",
    "        ....\n",
    "        #30\n",
    "            #1-30\n",
    "    #thanks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730dd8a9-481e-4d34-818a-d3dde81b9464",
   "metadata": {},
   "outputs": [],
   "source": [
    "for action in actions: \n",
    "    for sequence in range(0, no_sequences):\n",
    "        try: \n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3a8686-9a9f-4aae-bb39-edee9af3d353",
   "metadata": {},
   "source": [
    "## 5. Collect Keypoints value for Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4df9ce-1aee-4374-ad50-60acc542f853",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cap=cv2.VideoCapture(0)\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    for action in actions:\n",
    "        for sequence in range (no_sequences):\n",
    "            for frame_num in range (sequence_length):\n",
    "                #Read feed\n",
    "                ret, frame= cap.read()\n",
    "        \n",
    "                #Make detection\n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "                print(results)\n",
    "        \n",
    "                #Draw Lanmarks\n",
    "                draw_styled_landmark(image, results)\n",
    "\n",
    "                if frame_num == 0:\n",
    "                    cv2.putText(image,'STARRTING COLLECTION',(120,200),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),3,cv2.LINE_AA)\n",
    "                    cv2.putText(image,'COLLECTING FRAMES FOR:{} VIDEO NO:{}'.format(action,sequence),(15,20),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,0,255),1,cv2.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    cv2.imshow('Opencv Feed',image)\n",
    "                    cv2.waitKey(2000)\n",
    "                else:\n",
    "                    cv2.putText(image,'COLLECTING FRAMES FOR:{} VIDEO NO:{}'.format(action,sequence),(15,20),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,0,255),1,cv2.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    cv2.imshow('Opencv Feed',image)\n",
    "        \n",
    "                # NEW Export keypoints\n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "\n",
    "                #Break gracefully\n",
    "                if cv2.waitKey(10) & 0xFF==ord('q'):\n",
    "                    cap.release()\n",
    "                    cv2.destroyAllWindows()\n",
    "                    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feebebd31efbb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde7eb1c-c277-4dbc-b8e2-58ff324e7c30",
   "metadata": {},
   "source": [
    "## 6. Preprocess Data and Create Labels and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce706e6d-7915-4499-8166-a5428efbf988",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9b24f06-3af2-4928-a685-1238cd26e754",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faba3093-26b9-4dbf-b531-3399f0f363dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'hello'), (1, 'thanks'), (2, 'iloveyou')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(enumerate(actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af2cb739-4718-4a3b-86a4-354163630252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hello': 0, 'thanks': 1, 'iloveyou': 2}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8c5097-fcfb-4b4f-8dff-ca2cdd81deb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    for sequence in range (no_sequences):\n",
    "        window =[]\n",
    "        for frame_num in range(sequence_length):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc631fe-1026-4916-8550-f621b78352f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(sequences).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06a4fd8-eb53-4a9a-b32c-72279eb15e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(labels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bde6a2-cd94-4a10-aaa0-cbb4705a260d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bf75ff-f271-4a99-a648-89ca8205db14",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b273d7f1-47f0-4d43-9d1c-e6f4d3688086",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b432c7d-f6be-4a98-bac1-0c3ac12cae37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b8f0ec-cd05-4f5e-80ea-670a48865c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9d0a44-d9ee-45d7-aa3d-f863af64ea79",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7a3f13-f194-4910-857f-258f2ede6799",
   "metadata": {},
   "source": [
    "## 7. Build and Train LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbd46c0-0a5d-4319-8f00-8088ec751526",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f452f1-5d4d-488a-93fe-0171f196c678",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2578afd5-4ae1-4a1d-b087-5244221465c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30,1662)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7117cd-b01b-4876-952d-b2ff5d6befec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983418a8-6539-46dc-97a3-271065eb50ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "res=[0.2, 0.7, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0496b153-aa7a-4925-baf2-69e6bf694239",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions[np.argmax(res)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef6a67aa5ebc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=2000, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0635d9-0dff-4255-a797-9cbaed939f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760aba36-bcce-49ee-9d1f-1a4247cd99df",
   "metadata": {},
   "source": [
    "## 8. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21359e5-4815-4879-9ae6-3bb6a1920c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110c9926-2d9a-47d4-9cee-4f65dcd83872",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions[np.argmax(res[4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456b9398-b0e3-4808-b7f0-3578f28e02dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions[np.argmax(y_test[4])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b74ae83-adcb-4922-87ab-f4db9a2e5bec",
   "metadata": {},
   "source": [
    "## 9. Save the Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63c7133-e044-47a6-adbc-751254104d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('action1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f175bd-5ee0-4981-9651-a05a368dd01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35e9b48-76ee-41cc-8dd7-4157b84d47b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('action2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92228203-c91a-403b-b58c-08b260b70062",
   "metadata": {},
   "source": [
    "## 10. Evaluating using Confusion Matrix and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e1870c-d2a9-4058-ba51-e24056d2c879",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15611432-43df-4f2d-9c18-102e2d277f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788d3032-4c08-4972-a70d-8e8b1fd02642",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d872e6-39d2-461a-ae80-ed05cd99f4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d2114b-eedb-47bb-8e06-e86c51e4bdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f156a44e-d1ba-4bc9-8531-bf2c548d8f2a",
   "metadata": {},
   "source": [
    "## 11. Test in Real time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370b9142-d3ea-4beb-bb58-a5b46d1761a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmark(image, results):\n",
    "    #Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS,\n",
    "                              mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "                              mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1))\n",
    "    #Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                              mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2))\n",
    "    #Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=2),\n",
    "                              mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2))\n",
    "    #Draw right hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2),\n",
    "                              mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68490a3-d766-4892-a0cf-530f2c1a7c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4303394-609b-405a-9cd5-c00242105a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [(245,117,16), (117,245,16), (16,117,245)]\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfc915a-3c6c-4d6a-81e4-ccbedaf33220",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,18))\n",
    "plt.imshow(prob_viz(res, actions, image, colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e15b8c-1552-4030-b643-2c72a17d5830",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3, max_num_hands=2, )\n",
    "def mediapipe_detection (image):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(image)\n",
    "    return image, results\n",
    "\n",
    "\n",
    "label_name = []\n",
    "no = []\n",
    "Dataset_path = 'ISL_Dataset'\n",
    "KeyDataset_path = 'ISL_key_Dataset'\n",
    "for letter in os.listdir(Dataset_path):\n",
    "    for frame in os.listdir(os.path.join(Dataset_path, letter))[:1]:\n",
    "        image = cv2.imread(os.path.join(Dataset_path, letter, frame))\n",
    "        no.append(frame)\n",
    "        image, results = mediapipe_detection(image)\n",
    "        # draw(results)\n",
    "        \n",
    "        # plt.figure()\n",
    "        # plt.imshow(image)\n",
    "       \n",
    "    label_name.append(letter)\n",
    "print(label_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d723ba70-546b-492c-8565-1a29ced5b130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. New detection variables\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.7\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmark(image, results)\n",
    "        \n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_keypoints(results)\n",
    "        # sequence.insert(0,keypoints)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]\n",
    "        \n",
    "        if len(sequence) == 30:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            print(actions[np.argmax(res)])\n",
    "            predictions.append(np.argmax(res))\n",
    "            \n",
    "            \n",
    "        #3. Viz logic\n",
    "            if np.unique(predictions[-10:])[0]==np.argmax(res): \n",
    "                if res[np.argmax(res)] > threshold: \n",
    "                    \n",
    "                    if len(sentence) > 0: \n",
    "                        if actions[np.argmax(res)] != sentence[-1]:\n",
    "                            sentence.append(actions[np.argmax(res)])\n",
    "                    else:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "            if len(sentence) > 5: \n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "            # Viz probabilities\n",
    "            image = prob_viz(res, actions, image, colors)\n",
    "            \n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16d0e57-bea2-4271-9280-04fd624729db",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3b6d64-99fb-429b-97f3-e90bb6749ed6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
